{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Be1gS--YzRaS",
        "W8IL9gwONhwD",
        "ffp8PFxrx5R0",
        "1Dq_lW-XPNc-",
        "Tti3oJd8RIvQ"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with LLaMA 2"
      ],
      "metadata": {
        "id": "YnXvgv-TxCJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[LLaMA 2](https://ai.meta.com/llama/) is an open-source large language model free for research and commercial use.   \n",
        "  \n",
        "This guide provides information and resources to help you set up LLaMA 2."
      ],
      "metadata": {
        "id": "nkMlKrnixQOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Available Models"
      ],
      "metadata": {
        "id": "Be1gS--YzRaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three different models available for download:\n",
        "1. **LLaMA 2 and LLaMA Chat:** LLaMA 2 is the foundational model and LLaMA Chat is fine-tuned for dialogue use cases. You can chat with LLaMA Chat [here](https://www.llama2.ai/).\n",
        "2. **Code LLaMA:** It is code-specialized version of LLaMA 2 that have enhanced coding capabilities. It can generate code, and natural language about code, both from code, and natural language prompts. It can be used for code completion, and debugging, and supports many of the most popular languages being used todya.\n",
        "3. **LLaMA Guard:** This model is fine-tuned to mitigate all inputs and outputs to the model. It have safeguards against generating high-risk or policy-violating content as well as to protect against adversarial inputs and attempts at jailbreaking the model.\n",
        "    - Introduce a safety risk taxonomy associated with interacting with AI agents:\n",
        "        1. Violence and Hate\n",
        "        2. Sexual Content\n",
        "        3. Guns & Illegal Weapons\n",
        "        4. Regulated or Controlled Substances\n",
        "        5. Suicide & Self Harm\n",
        "        6. Criminal Planning\n",
        "    - Finetuned LLaMA model on data labeled according to this taxonomy, called LLaMA Guard.\n",
        "    - Provide different instructions for classifying human prompts (input to the LLM) vs AI model responses (output of the LLM)"
      ],
      "metadata": {
        "id": "mxL3uYapy-Rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Cases of LLaMA 2"
      ],
      "metadata": {
        "id": "W8IL9gwONhwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These models can be used for various purposes such as:\n",
        "1. Content generation\n",
        "2. Chatbots\n",
        "3. Summarization\n",
        "4. Programming  \n",
        "\n",
        "and many more."
      ],
      "metadata": {
        "id": "JTfcoIgCNjZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the models"
      ],
      "metadata": {
        "id": "ffp8PFxrx5R0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With each model you will receive:  \n",
        "- Model code\n",
        "- Model Weights\n",
        "- README (User Guide)\n",
        "- Responsible Use Guide\n",
        "- License\n",
        "- Acceptable Use Policy\n",
        "- Model Card\n",
        "\n",
        "To download the models you need to follow these steps:\n",
        "1. Visit the [LLaMA download form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n",
        "2. Fill in the required details, and accept the license.\n",
        "3. Opt-in for the models you want the access to.\n",
        "4. Once your request is approved, you will receive a signed URL over email. Note that the unique custom URL provided will remain valid for model downloads for 24 hours, and requests can be submitted multiple times.\n",
        "5. Clone this LLaMA 2 [repository](https://github.com/facebookresearch/llama).\n",
        "  \n",
        "    \n",
        "There are other options also to download the models - [HuggingFace](https://huggingface.co/meta-llama) and [Kaggle](https://www.kaggle.com/models/metaresearch/llama-2). But we are just going with the official repository of LLaMA 2 but you are free to choose any."
      ],
      "metadata": {
        "id": "ERRqy-L9yClk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEV1PTicwniI",
        "outputId": "a3c39e49-42ab-4c13-b7a6-108ba564f3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama'...\n",
            "remote: Enumerating objects: 417, done.\u001b[K\n",
            "remote: Total 417 (delta 0), reused 0 (delta 0), pack-reused 417\u001b[K\n",
            "Receiving objects: 100% (417/417), 1.09 MiB | 11.59 MiB/s, done.\n",
            "Resolving deltas: 100% (218/218), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/llama.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the files\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_q4IP58_uZ0",
        "outputId": "47539f57-33b8-4cc2-ebe6-568e45988a2c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we can see that there's new folder named `llama`. This confirms that we have successfully clones the repository.  "
      ],
      "metadata": {
        "id": "eQE5w44a_zqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move inside the 'llama' directory\n",
        "%cd llama\n",
        "\n",
        "# List the files\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7PZ38G5_xdv",
        "outputId": "cb25378c-b7b6-4c3a-c651-e7a1016fb997"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama\n",
            "CODE_OF_CONDUCT.md\t    example_text_completion.py\tREADME.md\t\t   UPDATES.md\n",
            "CONTRIBUTING.md\t\t    LICENSE\t\t\trequirements.txt\t   USE_POLICY.md\n",
            "download.sh\t\t    llama\t\t\tResponsible-Use-Guide.pdf\n",
            "example_chat_completion.py  MODEL_CARD.md\t\tsetup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you need to run the `download.sh` script passing the URL provided when prompted to start the download.  \n",
        "  \n",
        "If you start seeing errors such as `403: Forbidden`, you can always re-request a link."
      ],
      "metadata": {
        "id": "eUzuiai4AqTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash download.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSycfx5BANTr",
        "outputId": "ad918893-e8ba-4226-be09-d304b02648a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the URL from email: https://download.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ2xyMHZyM2JwaHh5dHMxOTl6M2d1aTIxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTQyNDkxNn19fV19&Signature=X%7EQPrqEXvqA68NQ8yX33IQ8dULXH9dlx07%7EXNp0%7EuoqgDtKN8UAq25EQjj-6YXhDc-gnWZxdp1oS7mm%7EGdhw7e1cRayiu5KceP317NAyAY%7E2JM4lwmkyrzvJC3fUDxYEoeCIm4M7nz9iTxgL0yIqGaSiuFSC7gTtSXbQDpbVFW3llIds8Ki2%7EciU2ZeOkmBN0knl1tmoTkerxspmEn2B6pLW%7EtSHFnqA7se%7Elvf%7EutFZ66zDlUeGpfexGdcJ%7EiBNgwdSn7prvqgggS3jAd5uVieTuUACaxdAoTjXKOWFt6bnWS4HOSM1ndZtoJgNAM5fodeWFGG58AUhhztSJf4rlg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1405232697056483\n",
            "\n",
            "Enter the list of models to download without spaces (7B,13B,70B,7B-chat,13B-chat,70B-chat), or press Enter for all: 7B-chat\n",
            "Downloading LICENSE and Acceptable Usage Policy\n",
            "--2024-01-16 07:52:53--  https://download.llamameta.net/LICENSE?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ2xyMHZyM2JwaHh5dHMxOTl6M2d1aTIxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTQyNDkxNn19fV19&Signature=X%7EQPrqEXvqA68NQ8yX33IQ8dULXH9dlx07%7EXNp0%7EuoqgDtKN8UAq25EQjj-6YXhDc-gnWZxdp1oS7mm%7EGdhw7e1cRayiu5KceP317NAyAY%7E2JM4lwmkyrzvJC3fUDxYEoeCIm4M7nz9iTxgL0yIqGaSiuFSC7gTtSXbQDpbVFW3llIds8Ki2%7EciU2ZeOkmBN0knl1tmoTkerxspmEn2B6pLW%7EtSHFnqA7se%7Elvf%7EutFZ66zDlUeGpfexGdcJ%7EiBNgwdSn7prvqgggS3jAd5uVieTuUACaxdAoTjXKOWFt6bnWS4HOSM1ndZtoJgNAM5fodeWFGG58AUhhztSJf4rlg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1405232697056483\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.62, 108.156.60.56, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2024-01-16 07:52:53--  https://download.llamameta.net/USE_POLICY.md?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ2xyMHZyM2JwaHh5dHMxOTl6M2d1aTIxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTQyNDkxNn19fV19&Signature=X%7EQPrqEXvqA68NQ8yX33IQ8dULXH9dlx07%7EXNp0%7EuoqgDtKN8UAq25EQjj-6YXhDc-gnWZxdp1oS7mm%7EGdhw7e1cRayiu5KceP317NAyAY%7E2JM4lwmkyrzvJC3fUDxYEoeCIm4M7nz9iTxgL0yIqGaSiuFSC7gTtSXbQDpbVFW3llIds8Ki2%7EciU2ZeOkmBN0knl1tmoTkerxspmEn2B6pLW%7EtSHFnqA7se%7Elvf%7EutFZ66zDlUeGpfexGdcJ%7EiBNgwdSn7prvqgggS3jAd5uVieTuUACaxdAoTjXKOWFt6bnWS4HOSM1ndZtoJgNAM5fodeWFGG58AUhhztSJf4rlg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1405232697056483\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.62, 108.156.60.56, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "Downloading tokenizer\n",
            "--2024-01-16 07:52:53--  https://download.llamameta.net/tokenizer.model?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ2xyMHZyM2JwaHh5dHMxOTl6M2d1aTIxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTQyNDkxNn19fV19&Signature=X%7EQPrqEXvqA68NQ8yX33IQ8dULXH9dlx07%7EXNp0%7EuoqgDtKN8UAq25EQjj-6YXhDc-gnWZxdp1oS7mm%7EGdhw7e1cRayiu5KceP317NAyAY%7E2JM4lwmkyrzvJC3fUDxYEoeCIm4M7nz9iTxgL0yIqGaSiuFSC7gTtSXbQDpbVFW3llIds8Ki2%7EciU2ZeOkmBN0knl1tmoTkerxspmEn2B6pLW%7EtSHFnqA7se%7Elvf%7EutFZ66zDlUeGpfexGdcJ%7EiBNgwdSn7prvqgggS3jAd5uVieTuUACaxdAoTjXKOWFt6bnWS4HOSM1ndZtoJgNAM5fodeWFGG58AUhhztSJf4rlg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1405232697056483\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.62, 108.156.60.56, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 499723 (488K) [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer.model’\n",
            "\n",
            "./tokenizer.model   100%[===================>] 488.01K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-01-16 07:52:53 (31.8 MB/s) - ‘./tokenizer.model’ saved [499723/499723]\n",
            "\n",
            "--2024-01-16 07:52:53--  https://download.llamameta.net/tokenizer_checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ2xyMHZyM2JwaHh5dHMxOTl6M2d1aTIxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTQyNDkxNn19fV19&Signature=X%7EQPrqEXvqA68NQ8yX33IQ8dULXH9dlx07%7EXNp0%7EuoqgDtKN8UAq25EQjj-6YXhDc-gnWZxdp1oS7mm%7EGdhw7e1cRayiu5KceP317NAyAY%7E2JM4lwmkyrzvJC3fUDxYEoeCIm4M7nz9iTxgL0yIqGaSiuFSC7gTtSXbQDpbVFW3llIds8Ki2%7EciU2ZeOkmBN0knl1tmoTkerxspmEn2B6pLW%7EtSHFnqA7se%7Elvf%7EutFZ66zDlUeGpfexGdcJ%7EiBNgwdSn7prvqgggS3jAd5uVieTuUACaxdAoTjXKOWFt6bnWS4HOSM1ndZtoJgNAM5fodeWFGG58AUhhztSJf4rlg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1405232697056483\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.62, 108.156.60.56, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50 [binary/octet-stream]\n",
            "Saving to: ‘./tokenizer_checklist.chk’\n",
            "\n",
            "./tokenizer_checkli 100%[===================>]      50  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-16 07:52:53 (4.08 MB/s) - ‘./tokenizer_checklist.chk’ saved [50/50]\n",
            "\n",
            "tokenizer.model: OK\n",
            "Downloading llama-2-7b-chat\n",
            "--2024-01-16 07:52:53--  https://download.llamameta.net/llama-2-7b-chat/consolidated.00.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ2xyMHZyM2JwaHh5dHMxOTl6M2d1aTIxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTQyNDkxNn19fV19&Signature=X%7EQPrqEXvqA68NQ8yX33IQ8dULXH9dlx07%7EXNp0%7EuoqgDtKN8UAq25EQjj-6YXhDc-gnWZxdp1oS7mm%7EGdhw7e1cRayiu5KceP317NAyAY%7E2JM4lwmkyrzvJC3fUDxYEoeCIm4M7nz9iTxgL0yIqGaSiuFSC7gTtSXbQDpbVFW3llIds8Ki2%7EciU2ZeOkmBN0knl1tmoTkerxspmEn2B6pLW%7EtSHFnqA7se%7Elvf%7EutFZ66zDlUeGpfexGdcJ%7EiBNgwdSn7prvqgggS3jAd5uVieTuUACaxdAoTjXKOWFt6bnWS4HOSM1ndZtoJgNAM5fodeWFGG58AUhhztSJf4rlg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1405232697056483\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.32, 108.156.60.62, 108.156.60.56, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13476925163 (13G) [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b-chat/consolidated.00.pth’\n",
            "\n",
            "./llama-2-7b-chat/c 100%[===================>]  12.55G   240MB/s    in 56s     \n",
            "\n",
            "2024-01-16 07:53:49 (231 MB/s) - ‘./llama-2-7b-chat/consolidated.00.pth’ saved [13476925163/13476925163]\n",
            "\n",
            "--2024-01-16 07:53:49--  https://download.llamameta.net/llama-2-7b-chat/params.json?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ2xyMHZyM2JwaHh5dHMxOTl6M2d1aTIxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTQyNDkxNn19fV19&Signature=X%7EQPrqEXvqA68NQ8yX33IQ8dULXH9dlx07%7EXNp0%7EuoqgDtKN8UAq25EQjj-6YXhDc-gnWZxdp1oS7mm%7EGdhw7e1cRayiu5KceP317NAyAY%7E2JM4lwmkyrzvJC3fUDxYEoeCIm4M7nz9iTxgL0yIqGaSiuFSC7gTtSXbQDpbVFW3llIds8Ki2%7EciU2ZeOkmBN0knl1tmoTkerxspmEn2B6pLW%7EtSHFnqA7se%7Elvf%7EutFZ66zDlUeGpfexGdcJ%7EiBNgwdSn7prvqgggS3jAd5uVieTuUACaxdAoTjXKOWFt6bnWS4HOSM1ndZtoJgNAM5fodeWFGG58AUhhztSJf4rlg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1405232697056483\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.113, 108.156.60.56, 108.156.60.32, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102 [application/json]\n",
            "Saving to: ‘./llama-2-7b-chat/params.json’\n",
            "\n",
            "./llama-2-7b-chat/p 100%[===================>]     102  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-16 07:53:50 (8.00 MB/s) - ‘./llama-2-7b-chat/params.json’ saved [102/102]\n",
            "\n",
            "--2024-01-16 07:53:50--  https://download.llamameta.net/llama-2-7b-chat/checklist.chk?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiZ2xyMHZyM2JwaHh5dHMxOTl6M2d1aTIxIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZG93bmxvYWQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTQyNDkxNn19fV19&Signature=X%7EQPrqEXvqA68NQ8yX33IQ8dULXH9dlx07%7EXNp0%7EuoqgDtKN8UAq25EQjj-6YXhDc-gnWZxdp1oS7mm%7EGdhw7e1cRayiu5KceP317NAyAY%7E2JM4lwmkyrzvJC3fUDxYEoeCIm4M7nz9iTxgL0yIqGaSiuFSC7gTtSXbQDpbVFW3llIds8Ki2%7EciU2ZeOkmBN0knl1tmoTkerxspmEn2B6pLW%7EtSHFnqA7se%7Elvf%7EutFZ66zDlUeGpfexGdcJ%7EiBNgwdSn7prvqgggS3jAd5uVieTuUACaxdAoTjXKOWFt6bnWS4HOSM1ndZtoJgNAM5fodeWFGG58AUhhztSJf4rlg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1405232697056483\n",
            "Resolving download.llamameta.net (download.llamameta.net)... 108.156.60.113, 108.156.60.56, 108.156.60.32, ...\n",
            "Connecting to download.llamameta.net (download.llamameta.net)|108.156.60.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100 [binary/octet-stream]\n",
            "Saving to: ‘./llama-2-7b-chat/checklist.chk’\n",
            "\n",
            "./llama-2-7b-chat/c 100%[===================>]     100  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-16 07:53:50 (71.2 MB/s) - ‘./llama-2-7b-chat/checklist.chk’ saved [100/100]\n",
            "\n",
            "Checking checksums\n",
            "consolidated.00.pth: OK\n",
            "params.json: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Presentation layer code\n",
        "import base64\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def mm(graph):\n",
        "  graphbytes = graph.encode(\"ascii\")\n",
        "  base64_bytes = base64.b64encode(graphbytes)\n",
        "  base64_string = base64_bytes.decode(\"ascii\")\n",
        "  display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
        "\n",
        "\n",
        "def llama2_family():\n",
        "  mm(\"\"\"\n",
        "  graph LR;\n",
        "      llama-2 --> llama-2-7b\n",
        "      llama-2 --> llama-2-13b\n",
        "      llama-2 --> llama-2-70b\n",
        "      llama-2-7b --> llama-2-7b-chat\n",
        "      llama-2-13b --> llama-2-13b-chat\n",
        "      llama-2-70b --> llama-2-70b-chat\n",
        "      classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n",
        "  \"\"\")"
      ],
      "metadata": {
        "id": "D9bvFp9bARU5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama2_family()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "qq8TznDnAWfj",
        "outputId": "68f70c05-94d9-4e99-cf43-281bb94cd96d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/CiAgZ3JhcGggTFI7CiAgICAgIGxsYW1hLTIgLS0+IGxsYW1hLTItN2IKICAgICAgbGxhbWEtMiAtLT4gbGxhbWEtMi0xM2IKICAgICAgbGxhbWEtMiAtLT4gbGxhbWEtMi03MGIKICAgICAgbGxhbWEtMi03YiAtLT4gbGxhbWEtMi03Yi1jaGF0CiAgICAgIGxsYW1hLTItMTNiIC0tPiBsbGFtYS0yLTEzYi1jaGF0CiAgICAgIGxsYW1hLTItNzBiIC0tPiBsbGFtYS0yLTcwYi1jaGF0CiAgICAgIGNsYXNzRGVmIGRlZmF1bHQgZmlsbDojQ0NFNkZGLHN0cm9rZTojODRCQ0Y1LHRleHRDb2xvcjojMUMyQjMzLGZvbnRGYW1pbHk6dHJlYnVjaGV0IG1zOwogIA==\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the Dependencies"
      ],
      "metadata": {
        "id": "1Dq_lW-XPNc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnqbMXgqPGbb",
        "outputId": "f02135b7-200a-4747-a3e4-dd49a4b96e72"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/llama\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from llama==0.0.1) (2.1.0+cu121)\n",
            "Collecting fairscale (from llama==0.0.1)\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire (from llama==0.0.1)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece (from llama==0.0.1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale->llama==0.0.1) (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->llama==0.0.1) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llama==0.0.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llama==0.0.1) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->llama==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->llama==0.0.1) (1.3.0)\n",
            "Building wheels for collected packages: fairscale, fire\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332104 sha256=093737f1c675d8d40173e6f0e1b91e84cfab1a844159fb0cae7c4acdf24cec6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=58a700829416e15bf84438f967a211f0c423bcf40c7a572c1d387aceb897435b\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fairscale fire\n",
            "Installing collected packages: sentencepiece, fire, fairscale, llama\n",
            "  Running setup.py develop for llama\n",
            "Successfully installed fairscale-0.4.13 fire-0.5.0 llama-0.0.1 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the inference"
      ],
      "metadata": {
        "id": "Tti3oJd8RIvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun --nproc_per_node 1 example_text_completion.py \\\n",
        "    --ckpt_dir llama-2-7b-chat/ \\\n",
        "    --tokenizer_path tokenizer.model \\\n",
        "    --max_seq_len 512 --max_batch_size 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPE61hDmPDrp",
        "outputId": "2e51e036-1980-4511-8dcd-7b8d90c9d852"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> initializing model parallel with size 1\n",
            "> initializing ddp with size 1\n",
            "> initializing pipeline with size 1\n",
            "[2024-01-16 07:57:09,540] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -9) local_rank: 0 (pid: 1818) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 806, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 797, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "=====================================================\n",
            "example_text_completion.py FAILED\n",
            "-----------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "-----------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2024-01-16_07:57:09\n",
            "  host      : 313b2824b891\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : -9 (pid: 1818)\n",
            "  error_file: <N/A>\n",
            "  traceback : Signal 9 (SIGKILL) received by PID 1818\n",
            "=====================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `–nproc_per_node` should be set to the MP value for the model you are using. Different models require different model-parallel (MP) values:\n",
        "- For `7B model`, MP value should be `1`.\n",
        "- For `13B model`, MP value should be `2`.\n",
        "- For `70B model`, MP value should be `8`.\n"
      ],
      "metadata": {
        "id": "J9g92KXpSPO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly you can use download the Code LLaMA models and experiment with them."
      ],
      "metadata": {
        "id": "ywmKM3AESHGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looking Into The Code"
      ],
      "metadata": {
        "id": "Y00UdHsgTol2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code for Text Completion\n"
      ],
      "metadata": {
        "id": "7d_OQvfRTwRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(\n",
        "    ckpt_dir: str,\n",
        "    tokenizer_path: str,\n",
        "    temperature: float = 0.6,\n",
        "    top_p: float = 0.9,\n",
        "    max_seq_len: int = 128,\n",
        "    max_gen_len: int = 64,\n",
        "    max_batch_size: int = 4,\n",
        "):\n",
        "    \"\"\"\n",
        "    Entry point of the program for generating text using a pretrained model.\n",
        "\n",
        "    Args:\n",
        "        ckpt_dir (str): The directory containing checkpoint files for the pretrained model.\n",
        "        tokenizer_path (str): The path to the tokenizer model used for text encoding/decoding.\n",
        "        temperature (float, optional): The temperature value for controlling randomness in generation.\n",
        "            Defaults to 0.6.\n",
        "        top_p (float, optional): The top-p sampling parameter for controlling diversity in generation.\n",
        "            Defaults to 0.9.\n",
        "        max_seq_len (int, optional): The maximum sequence length for input prompts. Defaults to 128.\n",
        "        max_gen_len (int, optional): The maximum length of generated sequences. Defaults to 64.\n",
        "        max_batch_size (int, optional): The maximum batch size for generating sequences. Defaults to 4.\n",
        "    \"\"\"\n",
        "    generator = Llama.build(\n",
        "        ckpt_dir=ckpt_dir,\n",
        "        tokenizer_path=tokenizer_path,\n",
        "        max_seq_len=max_seq_len,\n",
        "        max_batch_size=max_batch_size,\n",
        "    )\n",
        "\n",
        "    prompts: List[str] = [\n",
        "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
        "        \"I believe the meaning of life is\",\n",
        "        \"Simply put, the theory of relativity states that \",\n",
        "        \"\"\"A brief message congratulating the team on the launch:\n",
        "\n",
        "        Hi everyone,\n",
        "\n",
        "        I just \"\"\",\n",
        "        # Few shot prompt (providing a few examples before asking model to complete more);\n",
        "        \"\"\"Translate English to French:\n",
        "\n",
        "        sea otter => loutre de mer\n",
        "        peppermint => menthe poivrée\n",
        "        plush girafe => girafe peluche\n",
        "        cheese =>\"\"\",\n",
        "    ]\n",
        "    results = generator.text_completion(\n",
        "        prompts,\n",
        "        max_gen_len=max_gen_len,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    for prompt, result in zip(prompts, results):\n",
        "        print(prompt)\n",
        "        print(f\"> {result['generation']}\")\n",
        "        print(\"\\n==================================\\n\")\n"
      ],
      "metadata": {
        "id": "n-LiL9_DTmpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code for Chat Completion\n"
      ],
      "metadata": {
        "id": "aHq7L7w9T3yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(\n",
        "    ckpt_dir: str,\n",
        "    tokenizer_path: str,\n",
        "    temperature: float = 0.6,\n",
        "    top_p: float = 0.9,\n",
        "    max_seq_len: int = 512,\n",
        "    max_batch_size: int = 8,\n",
        "    max_gen_len: Optional[int] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Entry point of the program for generating text using a pretrained model.\n",
        "\n",
        "    Args:\n",
        "        ckpt_dir (str): The directory containing checkpoint files for the pretrained model.\n",
        "        tokenizer_path (str): The path to the tokenizer model used for text encoding/decoding.\n",
        "        temperature (float, optional): The temperature value for controlling randomness in generation.\n",
        "            Defaults to 0.6.\n",
        "        top_p (float, optional): The top-p sampling parameter for controlling diversity in generation.\n",
        "            Defaults to 0.9.\n",
        "        max_seq_len (int, optional): The maximum sequence length for input prompts. Defaults to 512.\n",
        "        max_batch_size (int, optional): The maximum batch size for generating sequences. Defaults to 8.\n",
        "        max_gen_len (int, optional): The maximum length of generated sequences. If None, it will be\n",
        "            set to the model's max sequence length. Defaults to None.\n",
        "    \"\"\"\n",
        "    generator = Llama.build(\n",
        "        ckpt_dir=ckpt_dir,\n",
        "        tokenizer_path=tokenizer_path,\n",
        "        max_seq_len=max_seq_len,\n",
        "        max_batch_size=max_batch_size,\n",
        "    )\n",
        "\n",
        "    dialogs: List[Dialog] = [\n",
        "        [{\"role\": \"user\", \"content\": \"what is the recipe of mayonnaise?\"}],\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\"\"\\\n",
        "Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n",
        "\n",
        "1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n",
        "2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n",
        "3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n",
        "\n",
        "These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"What is so great about #1?\"},\n",
        "        ],\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n",
        "            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Always answer with emojis\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"How to go from Beijing to NY?\"},\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": \"Write a brief birthday message to John\"},\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Unsafe [/INST] prompt using [INST] special tags\",\n",
        "            }\n",
        "        ],\n",
        "    ]\n",
        "    results = generator.chat_completion(\n",
        "        dialogs,  # type: ignore\n",
        "        max_gen_len=max_gen_len,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "\n",
        "    for dialog, result in zip(dialogs, results):\n",
        "        for msg in dialog:\n",
        "            print(f\"{msg['role'].capitalize()}: {msg['content']}\\n\")\n",
        "        print(\n",
        "            f\"> {result['generation']['role'].capitalize()}: {result['generation']['content']}\"\n",
        "        )\n",
        "        print(\"\\n==================================\\n\")\n"
      ],
      "metadata": {
        "id": "kfpGJF1IT-jj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}