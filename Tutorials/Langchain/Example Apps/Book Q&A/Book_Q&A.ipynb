{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8b715375",
      "metadata": {
        "id": "8b715375"
      },
      "source": [
        "# Top-K Similarity Search - Ask A Book A Question\n",
        "\n",
        "In this tutorial we will see a simple example of basic retrieval via Top-K Similarity search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d615a77",
      "metadata": {
        "id": "9d615a77"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2d3e92ed",
      "metadata": {
        "id": "2d3e92ed"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5166d759",
      "metadata": {
        "id": "5166d759"
      },
      "source": [
        "### Load your data\n",
        "\n",
        "Next let's load up some data. This process will only stage the loader, not actually load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b4a2d6bf",
      "metadata": {
        "id": "b4a2d6bf"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"./naval.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8d38044",
      "metadata": {
        "id": "c8d38044"
      },
      "source": [
        "Then let's go ahead and actually load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bcdac23c",
      "metadata": {
        "id": "bcdac23c"
      },
      "outputs": [],
      "source": [
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07a744a",
      "metadata": {
        "id": "e07a744a"
      },
      "source": [
        "Then let's actually check out what's been loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b4fd7c9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4fd7c9e",
        "outputId": "995c09bc-c4bd-4664-a92f-8f4674775649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 242 document(s) in your data\n",
            "There are 30 characters in your sample document\n",
            "Here is a sample: THE ALMANACK OF NAVAL RAVIKANT\n"
          ]
        }
      ],
      "source": [
        "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
        "print (f'You have {len(data)} document(s) in your data')\n",
        "print (f'There are {len(data[0].page_content)} characters in your sample document')\n",
        "print (f'Here is a sample: {data[0].page_content[:200]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8af9b604",
      "metadata": {
        "id": "8af9b604"
      },
      "source": [
        "### Chunk your data up into smaller documents\n",
        "\n",
        "While we could pass the entire book to a model w/ long context, we want to be picky about which information we share with our model.\n",
        "\n",
        "The first thing we'll do is chunk up our book into smaller pieces. The goal will be to take only a few of those smaller pieces and pass them to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fb3c6f02",
      "metadata": {
        "id": "fb3c6f02"
      },
      "outputs": [],
      "source": [
        "# We'll split our data into chunks around 500 characters each with a 50 character overlap. These are relatively small.\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "879873a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "879873a4",
        "outputId": "eab74bc4-9701-4e6d-ffcf-5c410d6e23ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now you have 685 documents\n"
          ]
        }
      ],
      "source": [
        "# Let's see how many small chunks we have\n",
        "print (f'Now you have {len(texts)} documents')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838b2843",
      "metadata": {
        "id": "838b2843"
      },
      "source": [
        "### Create embeddings of your documents to get ready for semantic search\n",
        "\n",
        "Next up we need to prepare for similarity searches. The way we do this is through embedding our documents (getting a vector per document).\n",
        "\n",
        "This will help us compare documents later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "373e695a",
      "metadata": {
        "id": "373e695a"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "884e7857",
      "metadata": {
        "id": "884e7857"
      },
      "source": [
        "Lets define OPNE API KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "42a1d5c3",
      "metadata": {
        "hide_input": false,
        "id": "42a1d5c3"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3205993a",
      "metadata": {
        "id": "3205993a"
      },
      "source": [
        "Then we'll get our embeddings using OpenAI's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4619d3a",
      "metadata": {
        "id": "b4619d3a"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76d66c06",
      "metadata": {
        "id": "76d66c06"
      },
      "source": [
        "Chroma\n",
        "\n",
        "\n",
        "First we'll pass our texts to Chroma via `.from_documents`, this will 1) embed the documents and get a vector, then 2) add them to the vectorstore for retrieval later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4e0d1c6a",
      "metadata": {
        "id": "4e0d1c6a"
      },
      "outputs": [],
      "source": [
        "# load it into Chroma\n",
        "vectorstore = Chroma.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4750ab",
      "metadata": {
        "id": "7d4750ab"
      },
      "source": [
        "Let's test it out. Let's see which documents are most closely related to a query.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "34929595",
      "metadata": {
        "id": "34929595"
      },
      "outputs": [],
      "source": [
        "query = \"How to earn money\"\n",
        "docs = vectorstore.similarity_search(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec60de1",
      "metadata": {
        "id": "1ec60de1"
      },
      "source": [
        "Then we can check them out. In theory, the texts which are deemed most similar should hold the answer to our question.\n",
        "But keep in mind that our query just happens to be a question, it could be a random statement or sentence and it would still work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e0f5b45",
      "metadata": {
        "id": "4e0f5b45"
      },
      "outputs": [],
      "source": [
        "# Here's an example of the first document that was returned\n",
        "for doc in docs:\n",
        "    print (f\"{doc.page_content}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c35dcd9",
      "metadata": {
        "id": "3c35dcd9"
      },
      "source": [
        "### Query those docs to get your answer back\n",
        "\n",
        "Those are just the docs which should hold our answer. Now we can pass those to a LangChain chain to query the LLM.\n",
        "\n",
        "We could do this manually, but a chain is a convenient helper for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f051337b",
      "metadata": {
        "id": "f051337b"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9b1c03",
      "metadata": {
        "id": "6b9b1c03"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f67ea7c2",
      "metadata": {
        "id": "f67ea7c2"
      },
      "outputs": [],
      "source": [
        "query = \"How to earn money?\"\n",
        "docs = vectorstore.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3dfd2b7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "3dfd2b7d",
        "outputId": "7592371c-c7f6-4191-f80b-3ffa0c8d9aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"To earn money, you need to provide society with something it wants but doesn't know how to get elsewhere. This can be done by delivering a product or service at scale. Another way is to have passive income that covers your expenses. Additionally, doing something you love can also lead to financial success.\\nSOURCES: ./naval.pdf\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=docs, question=\"Explain in 1 line\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0UHABQCim_wT",
        "outputId": "126b8a5f-223c-4b4f-e972-b2cf47676ee8"
      },
      "id": "0UHABQCim_wT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"To get rich without relying on luck, focus on delivering a product or service that society wants but doesn't know how to get elsewhere.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=docs, question=\"What have i asked?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TIgW7H6snIIC",
        "outputId": "3395d491-00ad-4fe9-82c6-9a2a28975864"
      },
      "id": "TIgW7H6snIIC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You have asked about how to get rich without getting lucky.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}