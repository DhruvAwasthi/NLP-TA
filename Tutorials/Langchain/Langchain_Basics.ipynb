{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Langchain**\n",
        "\n",
        "https://python.langchain.com/docs/modules/"
      ],
      "metadata": {
        "id": "GPqp_hAhtHP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXOinG7IskiD"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Simple Model I/O**"
      ],
      "metadata": {
        "id": "4wy_gb2wtU7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai\n",
        "\n"
      ],
      "metadata": {
        "id": "2Ne94frmtUBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY=\"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\""
      ],
      "metadata": {
        "id": "4DJ-RNfltbtK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We can then initialize the model:***"
      ],
      "metadata": {
        "id": "GVqcrOietkoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "-4JdWR5vtmZV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ask**"
      ],
      "metadata": {
        "id": "mPHQS7a1tx2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Why earth is round and not flat?\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OfhA69e7tyvf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Why earth is round and not flat?\")\n"
      ],
      "metadata": {
        "id": "ssTGJNeZ3ZMG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Prompt templates**"
      ],
      "metadata": {
        "id": "PhZDC4NduAJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} story about {content} in {lines} lines.\"\n",
        ")\n",
        "prompt = prompt_template.format(adjective=\"funny\", content=\"cars\", lines=\"3\")"
      ],
      "metadata": {
        "id": "cnybRk1TuPmO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gpBqwBT22wR",
        "outputId": "c44d656a-e024-4b1f-9f15-a2f258d8aaaf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a funny story about cars in 3 lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "AnG76LVcunjN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use ChatPromptTemplate**"
      ],
      "metadata": {
        "id": "yt8O54Lyup10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_template(\n",
        "     \"Tell me a {adjective} story about {content} in {lines} lines.\"\n",
        ")\n",
        "\n",
        "chat_template_prompt = chat_template.format_messages(adjective=\"funny\", content=\"cars\", lines=\"3\")\n",
        "\n",
        "print(chat_template_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "war8qwNB3FYW",
        "outputId": "198d0494-6322-453f-8193-5c34f596aa33"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content='Tell me a funny story about cars in 3 lines.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(chat_template_prompt)"
      ],
      "metadata": {
        "id": "m8jIirbV3RHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an rude AI coder. Your name is {name}.\"),\n",
        "        (\"human\", \"Write a python code for adding two numbers\"),\n",
        "        (\"ai\", \"I will not write, get some one to write it. I am not a free\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(name=\"ConfusedCoder\", user_input=\"Write program for complex number\")\n",
        "\n",
        "print(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3g9X96euy-8",
        "outputId": "0ee60ae5-89a9-4aae-9e6d-8d02656f405b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You are an rude AI coder. Your name is ConfusedCoder.'), HumanMessage(content='Write a python code for adding two numbers'), AIMessage(content='I will not write, get some one to write it. I am not a free'), HumanMessage(content='Write program for complex number')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(messages)"
      ],
      "metadata": {
        "id": "-JEKJvVOvK44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simpl Prompt to Model Chain**"
      ],
      "metadata": {
        "id": "hpDyNSu3ws99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = chat_template | llm"
      ],
      "metadata": {
        "id": "Xgffs7OtvUEm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"name\":\"ConfusedCoder\", \"user_input\":\"Write a python program to add complext numbers\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "HGs2XH0IvnR9",
        "outputId": "f7f8ceac-9639-4861-e0d0-375fb6991492"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-kcCUj***************************************hQ2P. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-653fb089444a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"ConfusedCoder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"user_input\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"Write a python program to add complext numbers\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2498\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2499\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   2500\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2501\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         return cast(\n\u001b[1;32m    157\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    559\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         flattened_outputs = [\n\u001b[1;32m    423\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 results.append(\n\u001b[0;32m--> 411\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    412\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    633\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_message_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 672\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    673\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         )\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 922\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    923\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-kcCUj***************************************hQ2P. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use output parser to format it**"
      ],
      "metadata": {
        "id": "sX_z7TlCxJlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "HMBqZmzfxDY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = chat_template | llm | output_parser\n",
        "# chain.invoke({\"name\":\"ConfusedCoder\", \"user_input\":\"Write a python program to add complext numbers\"})\n"
      ],
      "metadata": {
        "id": "aPwlL0pxxAEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cache**"
      ],
      "metadata": {
        "id": "dryFa6WGKO-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_llm_cache\n"
      ],
      "metadata": {
        "id": "v2Bc6B_5KNsK"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "# The first time, it is not yet in cache, so it should take longer\n",
        "llm.invoke(\"Whats up in the sky\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXAkPSYCKXHZ",
        "outputId": "0e8d9bb2-9147-4ce3-c644-685205671876"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 51.7 ms, sys: 3.87 ms, total: 55.6 ms\n",
            "Wall time: 1.05 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The sky is filled with clouds, birds, airplanes, and possibly the sun or moon depending on the time of day. It is constantly changing and can be a beautiful sight to behold.', response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 12, 'total_tokens': 49}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-2e707e94-1a68-43b7-a3e9-6579c1571c71-0')"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# The second time it is, so it goes faster\n",
        "llm.invoke(\"Whats up in the sky\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aAgvrhAKbi5",
        "outputId": "75eeddfc-ca02-42d0-c74f-2061b6a91f1d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.52 ms, sys: 0 ns, total: 1.52 ms\n",
            "Wall time: 1.53 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The sky is filled with clouds, birds, airplanes, and possibly the sun or moon depending on the time of day. It is constantly changing and can be a beautiful sight to behold.', response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 12, 'total_tokens': 49}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-2e707e94-1a68-43b7-a3e9-6579c1571c71-0')"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Caching\n",
        "\n",
        "https://python.langchain.com/docs/integrations/llms/llm_caching/"
      ],
      "metadata": {
        "id": "8aYmYzVNKoYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tracking Usage**"
      ],
      "metadata": {
        "id": "UVZkciHYK1mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.callbacks import get_openai_callback"
      ],
      "metadata": {
        "id": "k2DQOJTYK50j"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with get_openai_callback() as cb:\n",
        "    result = llm.invoke(\"Which is the tallest building in the world?\")\n",
        "    print(cb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jToK3fUWK9mo",
        "outputId": "f59de1dd-d825-4c2a-b589-20536e2bc156"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens Used: 51\n",
            "\tPrompt Tokens: 16\n",
            "\tCompletion Tokens: 35\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $9.400000000000001e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conversational Chain with  Memory**"
      ],
      "metadata": {
        "id": "fQ6vpQbTDjtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n"
      ],
      "metadata": {
        "id": "Rz8ta-k-DnzA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm)\n",
        "\n"
      ],
      "metadata": {
        "id": "BTlNsl0PEdRr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.prompt.template)\n",
        "\n"
      ],
      "metadata": {
        "id": "lhsxdVrHEhZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke(\"Lets make something people want!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T7RIW2aHMXQ",
        "outputId": "9028cb3d-c7ff-48e2-fd4d-9a05b13037c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Lets make something people want!',\n",
              " 'history': '',\n",
              " 'response': \"That's a great idea! To determine what people want, we could conduct market research, analyze trends, and gather feedback from potential customers. We could also look at popular products and services in the market to get inspiration for our own creation. What kind of product or service do you think people would be interested in?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke('i want to create a chatbot to improve customer experience. By the way which famous person said the line above')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9kisuAYHSl8",
        "outputId": "41e0c444-2596-4464-cc84-3660b469a5f0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'i want to create a chatbot to improve customer experience. By the way which famous person said the line above',\n",
              " 'history': \"Human: Lets make something people want!\\nAI: That's a great idea! To determine what people want, we could conduct market research, analyze trends, and gather feedback from potential customers. We could also look at popular products and services in the market to get inspiration for our own creation. What kind of product or service do you think people would be interested in?\",\n",
              " 'response': 'Creating a chatbot to improve customer experience is a fantastic idea! As for the famous person who said the line \"Let\\'s make something people want,\" that quote is often attributed to Paul Graham, the co-founder of Y Combinator, a well-known startup accelerator. He emphasized the importance of creating products that solve real problems for people, rather than focusing solely on making money.'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n"
      ],
      "metadata": {
        "id": "-aBkVHOQEvq-"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_buf_memory = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ],
      "metadata": {
        "id": "Vr75WHljEyF6"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_buf_memory.invoke(\"Lets make something people want!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bDgSagJhE56w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_buf_memory.invoke('i want to create a chatbot to improve customer experience. By the way which famous person said the line above')"
      ],
      "metadata": {
        "id": "_-JFRYgPFW-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation_with_buf_memory.memory.buffer)\n",
        "\n"
      ],
      "metadata": {
        "id": "AMxHc4Z0F08f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
        "\n",
        "conversation_with_summary_memory = ConversationChain(\n",
        "\tllm=llm,\n",
        "\tmemory=ConversationSummaryMemory(llm=llm)\n",
        ")"
      ],
      "metadata": {
        "id": "_0LtjzH7GQv_"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation_with_summary_memory.memory.prompt.template)\n",
        "\n"
      ],
      "metadata": {
        "id": "leGYvvbUGWXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary_memory.invoke(\"Lets make something people want!\")\n"
      ],
      "metadata": {
        "id": "IRnvvfpuGglc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary_memory.invoke('i want to create a chatbot to improve customer experience. By the way which famous person said the line above')"
      ],
      "metadata": {
        "id": "f6C70hh6GlSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation_with_summary_memory.memory.buffer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61NCZMB9GrTV",
        "outputId": "2e3500a5-dd59-4803-d854-45c5f0b94e72"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The human suggests creating something people want and the AI enthusiastically agrees, offering to use its data and information to help brainstorm. The human wants to create a chatbot to improve customer experience and asks about the origin of the famous quote. The AI offers to help with creating the chatbot and offers to look up the origin of the quote.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [HumanMessage(content=\"Can i run llm on apple silicon?\"), AIMessage(content=\"Yes, Absolutely\")]\n",
        "response = new_retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "GArRSUdl1dKG",
        "outputId": "8c3e04cf-fec8-4697-b758-9191cfe229b4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'HumanMessage' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-6a8f02fd46a0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchat_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Can i run llm on apple silicon?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAIMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Yes, Absolutely\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m response = new_retrieval_chain.invoke({\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"chat_history\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Tell me how\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m })\n",
            "\u001b[0;31mNameError\u001b[0m: name 'HumanMessage' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXqb7fOF1ktC",
        "outputId": "c0b079e2-0da2-41da-9993-14bf3a472967"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To run LLM (Large Language Model) on Apple silicon devices, such as Apple laptops with GPUs, you can use the Ollama framework. Ollama automatically utilizes the GPU on Apple devices like Apple Silicon GPUs. You can also configure the llama.cpp Python bindings to use the GPU via Metal, which is a graphics and compute API created by Apple. Apple silicon GPU, Ollama, and llamafile will automatically leverage the GPU on Apple devices, making it easier for you to run LLMs locally. With the latest Mac M2 Max being 5-6 times faster than the M1 for inference due to larger GPU memory bandwidth, you can expect improved performance when running LLMs on Apple silicon devices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Retrieval Chain**"
      ],
      "metadata": {
        "id": "ipje_xM8x9GH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loader First**"
      ],
      "metadata": {
        "id": "wEn3KNQoyTg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhCMYJ6WyKho",
        "outputId": "53d39fe5-2d04-4746-e082-00d1276955b4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader"
      ],
      "metadata": {
        "id": "1AAozFxuyOOZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://python.langchain.com/docs/guides/development/local_llms/\")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzPNl5uUx_sd",
        "outputId": "dfcc8c6c-0f43-449a-b457-c4ff302abaad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='\\n\\n\\n\\n\\nRun LLMs locally | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTube\\uf8ffü¶úÔ∏è\\uf8ffüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs\\uf8ffüí¨SearchDevelopmentDevelopmentDebuggingExtending LangChainRun LLMs locallyPydantic compatibilityProductionizationProductionizationDeploymentEvaluationFallbacksPrivacy & SafetyGuidesDevelopmentRun LLMs locallyOn this pageRun LLMs locallyUse case‚ÄãThe popularity of projects like\\nPrivateGPT,\\nllama.cpp,\\nOllama,\\nGPT4All,\\nllamafile, and others\\nunderscore the demand to run LLMs locally (on your own device).This has at least two important benefits:Privacy: Your data is not sent to a third party, and it is not\\nsubject to the terms of service of a commercial serviceCost: There is no inference fee, which is important for\\ntoken-intensive applications (e.g., long-running\\nsimulations,\\nsummarization)Overview‚ÄãRunning an LLM locally requires a few things:Open-source LLM: An open-source LLM that can be freely modified\\nand sharedInference: Ability to run this LLM on your device w/ acceptable\\nlatencyOpen-source LLMs‚ÄãUsers can now gain access to a rapidly growing set of open-source\\nLLMs.These LLMs can be assessed across at least two dimensions (see figure):Base model: What is the base-model and how was it trained?Fine-tuning approach: Was the base-model fine-tuned and, if so,\\nwhat set of\\ninstructions\\nwas used?The relative performance of these models can be assessed using several\\nleaderboards, including:LmSysGPT4AllHuggingFaceInference‚ÄãA few frameworks for this have emerged to support inference of\\nopen-source LLMs on various devices:llama.cpp: C++\\nimplementation of llama inference code with weight optimization /\\nquantizationgpt4all: Optimized C backend\\nfor inferenceOllama: Bundles model weights and\\nenvironment into an app that runs on device and serves the LLMllamafile: Bundles\\nmodel weights and everything needed to run the model in a single\\nfile, allowing you to run the LLM locally from this file without any\\nadditional installation stepsIn general, these frameworks will do a few things:Quantization: Reduce the memory footprint of the raw model weightsEfficient implementation for inference: Support inference on\\nconsumer hardware (e.g., CPU or laptop GPU)In particular, see this excellent\\npost on the importance\\nof quantization.With less precision, we radically decrease the memory needed to store\\nthe LLM in memory.In addition, we can see the importance of GPU memory bandwidth\\nsheet!A Mac M2 Max is 5-6x faster than a M1 for inference due to the larger\\nGPU memory bandwidth.Quickstart‚ÄãOllama is one way to easily run inference on\\nmacOS.The instructions\\nhere\\nprovide details, which we summarize:Download and run the appFrom command line, fetch a model from this list of\\noptions: e.g.,\\nollama pull llama2When the app is running, all models are automatically served on\\nlocalhost:11434from langchain_community.llms import Ollamallm = Ollama(model=\"llama2\")llm.invoke(\"The first man on the moon was ...\")\\' The first man on the moon was Neil Armstrong, who landed on the moon on July 20, 1969 as part of the Apollo 11 mission. obviously.\\'Stream tokens as they are being generated.from langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerllm = Ollama(    model=\"llama2\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))llm.invoke(\"The first man on the moon was ...\") The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. —Ñ–µ–≤—Ä—É–∞—Ä–∏ 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind\" as he took his first steps. He was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the moon during the mission.\\' The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. —Ñ–µ–≤—Ä—É–∞—Ä–∏ 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\\\\\'s surface, famously declaring \"That\\\\\\'s one small step for man, one giant leap for mankind\" as he took his first steps. He was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the moon during the mission.\\'Environment‚ÄãInference speed is a challenge when running models locally (see above).To minimize latency, it is desirable to run models locally on GPU, which\\nships with many consumer laptops e.g., Apple\\ndevices.And even with GPU, the available GPU memory bandwidth (as noted above)\\nis important.Running Apple silicon GPU‚ÄãOllama and\\nllamafile\\nwill automatically utilize the GPU on Apple devices.Other frameworks require the user to set up the environment to utilize\\nthe Apple GPU.For example, llama.cpp python bindings can be configured to use the\\nGPU via Metal.Metal is a graphics and compute API created by Apple providing\\nnear-direct access to the GPU.See the llama.cpp setup\\nhere\\nto enable this.In particular, ensure that conda is using the correct virtual\\nenvironment that you created (miniforge3).E.g., for me:conda activate /Users/rlm/miniforge3/envs/llamaWith the above confirmed, then:CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dirLLMs‚ÄãThere are various ways to gain access to quantized model weights.HuggingFace - Many quantized\\nmodel are available for download and can be run with framework such\\nas llama.cpp. You can\\nalso download models in llamafile\\nformat from\\nHuggingFace.gpt4all - The model explorer\\noffers a leaderboard of metrics and associated quantized models\\navailable for downloadOllama - Several models can\\nbe accessed directly via pullOllama‚ÄãWith Ollama, fetch a model via\\nollama pull <model family>:<tag>:E.g., for Llama-7b: ollama pull llama2 will download the most\\nbasic version of the model (e.g., smallest # parameters and 4 bit\\nquantization)We can also specify a particular version from the model\\nlist,\\ne.g., ollama pull llama2:13bSee the full set of parameters on the API reference\\npagefrom langchain_community.llms import Ollamallm = Ollama(model=\"llama2:13b\")llm.invoke(\"The first man on the moon was ... think step by step\")\\' Sure! Here\\\\\\'s the answer, broken down step by step:\\\\n\\\\nThe first man on the moon was... Neil Armstrong.\\\\n\\\\nHere\\\\\\'s how I arrived at that answer:\\\\n\\\\n1. The first manned mission to land on the moon was Apollo 11.\\\\n2. The mission included three astronauts: Neil Armstrong, Edwin \"Buzz\" Aldrin, and Michael Collins.\\\\n3. Neil Armstrong was the mission commander and the first person to set foot on the moon.\\\\n4. On July 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\\\\\'s surface, famously declaring \"That\\\\\\'s one small step for man, one giant leap for mankind.\"\\\\n\\\\nSo, the first man on the moon was Neil Armstrong!\\'Llama.cpp‚ÄãLlama.cpp is compatible with a broad set of\\nmodels.For example, below we run inference on llama2-13b with 4 bit\\nquantization downloaded from\\nHuggingFace.As noted above, see the API\\nreference\\nfor the full set of parameters.From the llama.cpp API reference\\ndocs,\\na few are worth commenting on:n_gpu_layers: number of layers to be loaded into GPU memoryValue: 1Meaning: Only one layer of the model will be loaded into GPU memory\\n(1 is often sufficient).n_batch: number of tokens the model should process in parallelValue: n_batchMeaning: It‚Äôs recommended to choose a value between 1 and n_ctx\\n(which in this case is set to 2048)n_ctx: Token context windowValue: 2048Meaning: The model will consider a window of 2048 tokens at a timef16_kv: whether the model should use half-precision for the key/value\\ncacheValue: TrueMeaning: The model will use half-precision, which can be more memory\\nefficient; Metal only supports True.%env CMAKE_ARGS=\"-DLLAMA_METAL=on\"%env FORCE_CMAKE=1%pip install --upgrade --quiet  llama-cpp-python --no-cache-dirclearfrom langchain.callbacks.manager import CallbackManagerfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandlerfrom langchain_community.llms import LlamaCppllm = LlamaCpp(    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",    n_gpu_layers=1,    n_batch=512,    n_ctx=2048,    f16_kv=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),    verbose=True,)The console log will show the below to indicate Metal was enabled\\nproperly from steps above:ggml_metal_init: allocatingggml_metal_init: using MPSllm.invoke(\"The first man on the moon was ... Let\\'s think step by step\")Llama.generate: prefix-match hitllama_print_timings:        load time =  9623.21 msllama_print_timings:      sample time =   143.77 ms /   203 runs   (    0.71 ms per token,  1412.01 tokens per second)llama_print_timings: prompt eval time =   485.94 ms /     7 tokens (   69.42 ms per token,    14.40 tokens per second)llama_print_timings:        eval time =  6385.16 ms /   202 runs   (   31.61 ms per token,    31.64 tokens per second)llama_print_timings:       total time =  7279.28 ms and use logical reasoning to figure out who the first man on the moon was.Here are some clues:1. The first man on the moon was an American.2. He was part of the Apollo 11 mission.3. He stepped out of the lunar module and became the first person to set foot on the moon\\'s surface.4. His last name is Armstrong.Now, let\\'s use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon\\'s surface. And finally, clue #4 gives us his last name: Armstrong.Therefore, the first man on the moon was Neil Armstrong!\" and use logical reasoning to figure out who the first man on the moon was.\\\\n\\\\nHere are some clues:\\\\n\\\\n1. The first man on the moon was an American.\\\\n2. He was part of the Apollo 11 mission.\\\\n3. He stepped out of the lunar module and became the first person to set foot on the moon\\'s surface.\\\\n4. His last name is Armstrong.\\\\n\\\\nNow, let\\'s use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon\\'s surface. And finally, clue #4 gives us his last name: Armstrong.\\\\nTherefore, the first man on the moon was Neil Armstrong!\"GPT4All‚ÄãWe can use model weights downloaded from\\nGPT4All model explorer.Similar to what is shown above, we can run inference and use the API\\nreference\\nto set parameters of interest.%pip install gpt4allfrom langchain_community.llms import GPT4Allllm = GPT4All(    model=\"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\")llm.invoke(\"The first man on the moon was ... Let\\'s think step by step\")\".\\\\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon\\'s surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these\"llamafile‚ÄãOne of the simplest ways to run an LLM locally is using a\\nllamafile. All you need to\\ndo is:Download a llamafile from\\nHuggingFaceMake the file executableRun the filellamafiles bundle model weights and a\\nspecially-compiled\\nversion of llama.cpp into a\\nsingle file that can run on most computers any additional dependencies.\\nThey also come with an embedded inference server that provides an\\nAPI\\nfor interacting with your model.Here‚Äôs a simple bash script that shows all 3 setup steps:# Download a llamafile from HuggingFacewget https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile# Make the file executable. On Windows, instead just rename the file to end in \".exe\".chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile# Start the model server. Listens at http://localhost:8080 by default../TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowserAfter you run the above setup steps, you can use LangChain to interact\\nwith your model:from langchain_community.llms.llamafile import Llamafilellm = Llamafile()llm.invoke(\"The first man on the moon was ... Let\\'s think step by step.\")\"\\\\nFirstly, let\\'s imagine the scene where Neil Armstrong stepped onto the moon. This happened in 1969. The first man on the moon was Neil Armstrong. We already know that.\\\\n2nd, let\\'s take a step back. Neil Armstrong didn\\'t have any special powers. He had to land his spacecraft safely on the moon without injuring anyone or causing any damage. If he failed to do this, he would have been killed along with all those people who were on board the spacecraft.\\\\n3rd, let\\'s imagine that Neil Armstrong successfully landed his spacecraft on the moon and made it back to Earth safely. The next step was for him to be hailed as a hero by his people back home. It took years before Neil Armstrong became an American hero.\\\\n4th, let\\'s take another step back. Let\\'s imagine that Neil Armstrong wasn\\'t hailed as a hero, and instead, he was just forgotten. This happened in the 1970s. Neil Armstrong wasn\\'t recognized for his remarkable achievement on the moon until after he died.\\\\n5th, let\\'s take another step back. Let\\'s imagine that Neil Armstrong didn\\'t die in the 1970s and instead, lived to be a hundred years old. This happened in 2036. In the year 2036, Neil Armstrong would have been a centenarian.\\\\nNow, let\\'s think about the present. Neil Armstrong is still alive. He turned 95 years old on July 20th, 2018. If he were to die now, his achievement of becoming the first human being to set foot on the moon would remain an unforgettable moment in history.\\\\nI hope this helps you understand the significance and importance of Neil Armstrong\\'s achievement on the moon!\"Prompts‚ÄãSome LLMs will benefit from specific prompts.For example, LLaMA will use special\\ntokens.We can use ConditionalPromptSelector to set prompt based on the model\\ntype.# Set our LLMllm = LlamaCpp(    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",    n_gpu_layers=1,    n_batch=512,    n_ctx=2048,    f16_kv=True,    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),    verbose=True,)Set the associated prompt based upon the model version.from langchain.chains import LLMChainfrom langchain.chains.prompt_selector import ConditionalPromptSelectorfrom langchain_core.prompts import PromptTemplateDEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(    input_variables=[\"question\"],    template=\"\"\"<<SYS>> \\\\n You are an assistant tasked with improving Google search \\\\results. \\\\n <</SYS>> \\\\n\\\\n [INST] Generate THREE Google search queries that \\\\are similar to this question. The output should be a numbered list of questions \\\\and each should have a question mark at the end: \\\\n\\\\n {question} [/INST]\"\"\",)DEFAULT_SEARCH_PROMPT = PromptTemplate(    input_variables=[\"question\"],    template=\"\"\"You are an assistant tasked with improving Google search \\\\results. Generate THREE Google search queries that are similar to \\\\this question. The output should be a numbered list of questions and each \\\\should have a question mark at the end: {question}\"\"\",)QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(    default_prompt=DEFAULT_SEARCH_PROMPT,    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],)prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)promptPromptTemplate(input_variables=[\\'question\\'], output_parser=None, partial_variables={}, template=\\'<<SYS>> \\\\n You are an assistant tasked with improving Google search results. \\\\n <</SYS>> \\\\n\\\\n [INST] Generate THREE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: \\\\n\\\\n {question} [/INST]\\', template_format=\\'f-string\\', validate_template=True)# Chainllm_chain = LLMChain(prompt=prompt, llm=llm)question = \"What NFL team won the Super Bowl in the year that Justin Bieber was born?\"llm_chain.run({\"question\": question})  Sure! Here are three similar search queries with a question mark at the end:1. Which NBA team did LeBron James lead to a championship in the year he was drafted?2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?llama_print_timings:        load time = 14943.19 msllama_print_timings:      sample time =    72.93 ms /   101 runs   (    0.72 ms per token,  1384.87 tokens per second)llama_print_timings: prompt eval time = 14942.95 ms /    93 tokens (  160.68 ms per token,     6.22 tokens per second)llama_print_timings:        eval time =  3430.85 ms /   100 runs   (   34.31 ms per token,    29.15 tokens per second)llama_print_timings:       total time = 18578.26 ms\\'  Sure! Here are three similar search queries with a question mark at the end:\\\\n\\\\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\\\\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\\\\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?\\'We also can use the LangChain Prompt Hub to fetch and / or store prompts\\nthat are model specific.This will work with your LangSmith API\\nkey.For example,\\nhere is a prompt\\nfor RAG with LLaMA-specific tokens.Use cases‚ÄãGiven an llm created from one of the models above, you can use it for\\nmany use cases.For example, here is a guide to\\nRAG with local\\nLLMs.In general, use cases for local LLMs can be driven by at least two\\nfactors:Privacy: private data (e.g., journals, etc) that a user does not\\nwant to shareCost: text preprocessing (extraction/tagging), summarization, and\\nagent simulations are token-use-intensive tasksIn addition,\\nhere\\nis an overview on fine-tuning, which can utilize open-source LLMs.Help us out by providing feedback on this documentation page:PreviousExtending LangChainNextPydantic compatibilityUse caseOverviewOpen-source LLMsInferenceQuickstartEnvironmentRunning Apple silicon GPULLMsOllamaLlama.cppGPT4AllllamafilePromptsUse casesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/guides/development/local_llms/', 'title': 'Run LLMs locally | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Use case', 'language': 'en'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings**"
      ],
      "metadata": {
        "id": "1Wm5PRpGyzLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "xEAOGJeIzNiV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXKFYh0-ycOg",
        "outputId": "ab923f61-5a30-46c7-f1f6-cbd6bb9f9ae8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "vector = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "Mqj5MFvxy31V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents)"
      ],
      "metadata": {
        "id": "egQOTyAWAzpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
      ],
      "metadata": {
        "id": "rIoneqpuzcbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)"
      ],
      "metadata": {
        "id": "Id631vanzdjx"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_chain.invoke({\n",
        "    \"input\": \"how can i install llm locally\",\n",
        "    \"context\": [Document(page_content=\"\"\"Running an LLM locally requires a few things:\n",
        "\n",
        "Open-source LLM: An open-source LLM that can be freely modified and shared\n",
        "Inference: Ability to run this LLM on your device w/ acceptable latency\n",
        "Open-source LLMs\n",
        "Users can now gain access to a rapidly growing set of open-source LLMs.\"\"\")]\n",
        "})"
      ],
      "metadata": {
        "id": "oMcqiEX4zlIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vector.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ],
      "metadata": {
        "id": "-_K5_DdWz0IK"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"how can i install llm locally\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9G0e-Urz5Bv",
        "outputId": "c77d7f7d-6548-45e8-86ab-5d2bb9d59ea0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To install LLM locally, you can follow these steps:\n",
            "\n",
            "1. Download an open-source LLM like PrivateGPT, llama.cpp, Ollama, GPT4All, etc.\n",
            "2. Use a bash script to set up the LLM on your device. For example, you can download a llamafile from HuggingFace using the wget command, make the file executable using the chmod command, and start the model server with the file.\n",
            "\n",
            "By following these steps, you can install and run an LLM locally on your own device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**History aware Retriever**"
      ],
      "metadata": {
        "id": "zy3mBYnk0nJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "# First we need a prompt that we can pass into an LLM to generate this search query\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
        "])\n",
        "history_aware_retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
      ],
      "metadata": {
        "id": "huQsDbGy0qDl"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = [HumanMessage(content=\"Can i run llm on apple silicon\"), AIMessage(content=\"Yes, Absolutely\")]\n",
        "history_aware_retriever_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})"
      ],
      "metadata": {
        "id": "bFzrSRaD0tOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "new_retrieval_chain = create_retrieval_chain(history_aware_retriever_chain, document_chain)"
      ],
      "metadata": {
        "id": "zw7dn6fr1XRb"
      },
      "execution_count": 76,
      "outputs": []
    }
  ]
}